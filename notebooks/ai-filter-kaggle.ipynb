{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 3364057,
     "sourceType": "datasetVersion",
     "datasetId": 2029045
    },
    {
     "sourceId": 8915300,
     "sourceType": "datasetVersion",
     "datasetId": 5361384
    },
    {
     "sourceId": 8915337,
     "sourceType": "datasetVersion",
     "datasetId": 5361410
    },
    {
     "sourceId": 8915570,
     "sourceType": "datasetVersion",
     "datasetId": 5361587
    }
   ],
   "dockerImageVersionId": 30733,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Handle Data",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "code",
   "source": "pip install dlib",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:07:11.559653Z",
     "iopub.execute_input": "2024-07-09T14:07:11.560023Z",
     "iopub.status.idle": "2024-07-09T14:14:46.782901Z",
     "shell.execute_reply.started": "2024-07-09T14:07:11.559996Z",
     "shell.execute_reply": "2024-07-09T14:14:46.781692Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport numpy as np\nimport torch\nimport dlib\nimport cv2 as cv",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:15:30.723360Z",
     "iopub.execute_input": "2024-07-09T14:15:30.724247Z",
     "iopub.status.idle": "2024-07-09T14:15:35.968363Z",
     "shell.execute_reply.started": "2024-07-09T14:15:30.724207Z",
     "shell.execute_reply": "2024-07-09T14:15:35.967312Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## XML Tree to get data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import xml.etree.ElementTree as ET\ntree = ET.parse('/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W.xml')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:15:38.224571Z",
     "iopub.execute_input": "2024-07-09T14:15:38.225330Z",
     "iopub.status.idle": "2024-07-09T14:15:41.509178Z",
     "shell.execute_reply.started": "2024-07-09T14:15:38.225287Z",
     "shell.execute_reply": "2024-07-09T14:15:41.508372Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "root = tree.getroot()\n\nfor child in root:\n    print(f\"{child.tag} + {child.attrib}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:15:43.746795Z",
     "iopub.execute_input": "2024-07-09T14:15:43.747197Z",
     "iopub.status.idle": "2024-07-09T14:15:43.752738Z",
     "shell.execute_reply.started": "2024-07-09T14:15:43.747169Z",
     "shell.execute_reply": "2024-07-09T14:15:43.751885Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport os\n\nclass IDataset:\n    def __init__(self, data_dir):\n        self.data_dir = data_dir\n        self.images, self.landmarks = self.generateData(self.data_dir)\n\n    def sample_image(self, image):\n        \"\"\"\n        Create a dictionary containing image details.\n\n        Args:\n            image: An XML element containing image data.\n\n        Returns:\n            A dictionary with image attributes including:\n            - filename (str)\n            - width (int)\n            - height (int)\n            - box_top (int)\n            - box_left (int)\n            - box_width (int)\n            - box_height (int)\n            - landmarks (np.ndarray of tuples): Each tuple contains (x, y) coordinates of a landmark.\n        \"\"\"\n\n        image_result = {}\n        image_result['filename'] = image.attrib['file']\n        image_result['width'] = int(image.attrib['width'])\n        image_result['height'] = int(image.attrib['height'])\n\n        box = image.find('box')\n        image_result['box_top'] = int(box.attrib['top'])\n        image_result['box_left'] = int(box.attrib['left'])\n        image_result['box_width'] = int(box.attrib['width'])\n        image_result['box_height'] = int(box.attrib['height'])\n\n        # set up landmarks\n        landmarks = np.array([[float(part.attrib[\"x\"]), float(part.attrib[\"y\"])] for part in box])\n        image_result['landmarks'] = landmarks\n\n        return image_result\n\n    def create_samples_xml(self, xml_file_path):\n        tree = ET.parse(xml_file_path)\n        root = tree.getroot()\n        images = root.find('images')\n        samples: list[dict] = [self.sample_image(image) for image in images]\n        return samples\n\n    def get_data(self, samples, root_dir):\n        images = []\n        landmarks_list = []\n        samples = tqdm(samples)\n        for sample in samples:\n            image_path = os.path.join(root_dir, sample['filename'])\n            if os.path.exists(image_path):\n                image = Image.open(image_path).convert(\"RGB\")\n                width = sample['width']\n                height = sample['height']\n                box_left = sample[\"box_left\"]\n                box_top = sample[\"box_top\"]\n                box_width = sample[\"box_width\"]\n                box_height = sample[\"box_height\"]\n                landmarks = sample['landmarks']\n                crop_image = image.crop((box_left, box_top, box_left + box_width, box_top + box_height))\n                landmarks -= np.array([box_left, box_top])\n                images.append(crop_image)\n                landmarks_list.append(landmarks)\n        return images, landmarks_list\n\n    def generateData(self, path):\n        samples = self.create_samples_xml(path)\n        images, landmarks = self.get_data(samples, os.path.dirname(path))\n        return images, landmarks\n\nclass LandmarkData(Dataset):\n    def __init__(self, images: torch.Tensor, landmarks: torch.Tensor):\n        self.images = images\n        self.landmarks = landmarks\n\n    def __len__(self):\n        return len(self.landmarks)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.landmarks[idx]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:15:48.550224Z",
     "iopub.execute_input": "2024-07-09T14:15:48.551181Z",
     "iopub.status.idle": "2024-07-09T14:15:48.569182Z",
     "shell.execute_reply.started": "2024-07-09T14:15:48.551149Z",
     "shell.execute_reply": "2024-07-09T14:15:48.568169Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data_train = IDataset(\"/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\")\ndata_test = IDataset(\"/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:15:53.947196Z",
     "iopub.execute_input": "2024-07-09T14:15:53.947925Z",
     "iopub.status.idle": "2024-07-09T14:19:16.128028Z",
     "shell.execute_reply.started": "2024-07-09T14:15:53.947893Z",
     "shell.execute_reply": "2024-07-09T14:19:16.127093Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Agument",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:01:50.270647Z",
     "iopub.execute_input": "2024-07-09T14:01:50.271468Z",
     "iopub.status.idle": "2024-07-09T14:01:50.593813Z",
     "shell.execute_reply.started": "2024-07-09T14:01:50.271435Z",
     "shell.execute_reply": "2024-07-09T14:01:50.592674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport numpy as np\nimport torch\n\ntransform_train = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n    A.Resize(height=256, width=256, always_apply=True),\n    A.RandomCrop(height=224, width=224, always_apply=True),\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n    A.RandomBrightnessContrast(p=0.5),\n#     A.Cutout(num_holes=8, max_h_size=18, max_w_size=18, p=0.5),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n\n\ntransform_test = A.Compose([\n    A.Resize(height=256, width=256, always_apply=True),\n    A.CenterCrop(height=224, width=224, always_apply=True),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n\n\nclass DataAugment():\n    def __init__(self):\n        self.transform_train = transform_train\n        self.transform_test = transform_test\n        self.images = None\n        self.landmarks = None\n\n    def norm_transform_dataset(self, images, landmarks, transform):\n        final_images = []\n        final_landmarks = []\n\n        for i in tqdm(range(len(images))):\n            img = images[i]\n            ldm = landmarks[i]\n\n            img = np.array(img)\n\n            # transform\n            transformed = transform(image=img, keypoints=ldm)\n            transformed_img = transformed['image']\n            transformed_lmd = transformed['keypoints']\n\n            # normalize\n            color_channels, height, width = transformed_img.shape\n            transformed_lmd = transformed_lmd / np.array([width, height]) - 0.5\n            transformed_lmd = torch.tensor(transformed_lmd, dtype=torch.float32)\n\n            final_images.append(transformed_img)\n            final_landmarks.append(transformed_lmd)\n\n        self.images = final_images\n        self.landmarks = final_landmarks\n\n        return final_images, final_landmarks\n\n    def finish_data_tensor(self, images, landmarks):\n        final_images_train_converted = torch.stack([tensor.permute(1, 2, 0) for tensor in images])\n        final_landmarks_train = np.array(landmarks)\n\n        return final_images_train_converted, final_landmarks_train",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:20:13.404837Z",
     "iopub.execute_input": "2024-07-09T14:20:13.405217Z",
     "iopub.status.idle": "2024-07-09T14:20:13.420181Z",
     "shell.execute_reply.started": "2024-07-09T14:20:13.405189Z",
     "shell.execute_reply": "2024-07-09T14:20:13.419251Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Agument Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "DA = DataAugment()\n# train data\nimages_train, landmarks_train = data_train.images, data_train.landmarks\nimages_train_transform, landmarks_train_transform = DA.norm_transform_dataset(images_train, landmarks_train,\n                                                                               DA.transform_train)\ntrain_set_images, train_set_landmarks = images_train_transform, landmarks_train_transform\n\n# test data\nimages_test, landmarks_test = data_test.images, data_test.landmarks\nimages_test_transform, landmarks_test_transform = DA.norm_transform_dataset(images_test, landmarks_test, DA.transform_test)\ntest_set_images, test_set_landmarks = images_test_transform, landmarks_test_transform",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:20:15.292422Z",
     "iopub.execute_input": "2024-07-09T14:20:15.292793Z",
     "iopub.status.idle": "2024-07-09T14:20:49.496097Z",
     "shell.execute_reply.started": "2024-07-09T14:20:15.292763Z",
     "shell.execute_reply": "2024-07-09T14:20:49.495148Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data for model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader\n# trains data\ndata_train = LandmarkData(train_set_images, train_set_landmarks)\ntrain_set = DataLoader(data_train, batch_size=32, shuffle=True, num_workers=4)\n# test data\ndata_test = LandmarkData(test_set_images, test_set_landmarks)\ntest_set = DataLoader(data_test, batch_size=32, shuffle=False, num_workers=4)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:21:28.944835Z",
     "iopub.execute_input": "2024-07-09T14:21:28.945292Z",
     "iopub.status.idle": "2024-07-09T14:21:28.951922Z",
     "shell.execute_reply.started": "2024-07-09T14:21:28.945263Z",
     "shell.execute_reply": "2024-07-09T14:21:28.950732Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Model EfficentnetB0",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch.nn as nn\nimport torchvision.models as models\nclass EfficientNetB0(nn.Module):\n    def __init__(self, num_points):\n        super(EfficientNetB0, self).__init__()\n        self.num_points = num_points\n        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(p=0.2, inplace=True),\n            nn.Linear(self.model.classifier[1].in_features, 256),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(256, num_points * 2)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(-1, self.num_points, 2)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:21:55.280385Z",
     "iopub.execute_input": "2024-07-09T14:21:55.280778Z",
     "iopub.status.idle": "2024-07-09T14:21:56.772986Z",
     "shell.execute_reply.started": "2024-07-09T14:21:55.280747Z",
     "shell.execute_reply": "2024-07-09T14:21:56.772194Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Train function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.optim import Adam\nfrom torch.nn import MSELoss\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef train_model(model, train_data, test_data, num_epochs=35):\n    best_test_loss = 1e9\n    train_loss_history = []\n    test_loss_history = []\n    loss_fn = MSELoss()\n    optimizer = Adam(params=model.model.classifier.parameters(), lr=1e-3)\n    for epoch in range(num_epochs):\n        # Train model\n        model.train()\n        running_loss = 0.0\n        train_pbar = tqdm(train_data)\n\n        for images, landmarks in train_pbar:\n            images = images.to(DEVICE)\n            landmarks = landmarks.to(DEVICE)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = loss_fn(outputs, landmarks)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n            train_pbar.set_postfix({\"Train loss\": loss.item()})\n\n        train_loss = running_loss / len(train_data.dataset)\n        train_loss_history.append(train_loss)\n\n        # test model\n        model.eval()\n        test_loss = 0.0\n        test_pbar = tqdm(test_data)\n\n        with torch.no_grad():\n            for images, landmarks in test_pbar:\n                images = images.to(DEVICE)\n                landmarks = landmarks.to(DEVICE)\n\n                outputs = model(images)\n                loss = loss_fn(outputs, landmarks)\n\n                test_loss += loss.item() * images.size(0)\n                test_pbar.set_postfix({\"Test loss\": loss.item()})\n\n        test_loss = test_loss / len(test_data.dataset)\n        test_loss_history.append(test_loss)\n\n        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n\n        if test_loss < best_test_loss:\n            best_test_loss = test_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(\"Model saving ...\")\n\n    return model\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:22:45.980399Z",
     "iopub.execute_input": "2024-07-09T14:22:45.981435Z",
     "iopub.status.idle": "2024-07-09T14:22:46.027221Z",
     "shell.execute_reply.started": "2024-07-09T14:22:45.981400Z",
     "shell.execute_reply": "2024-07-09T14:22:46.026111Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model = EfficientNetB0(68)\nmodel.to(DEVICE)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:55:18.240387Z",
     "iopub.execute_input": "2024-07-09T14:55:18.240780Z",
     "iopub.status.idle": "2024-07-09T14:55:18.451150Z",
     "shell.execute_reply.started": "2024-07-09T14:55:18.240748Z",
     "shell.execute_reply": "2024-07-09T14:55:18.450117Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for param in model.model.parameters():\n    param.requires_grad = False\n\nfor param in model.model.classifier.parameters():\n    param.requires_grad = True\n#     summary(model, input_size=(3, 224, 224))\n#     model.load_state_dict(torch.load('best_model_2.pth'))\nmodel = train_model(model, train_set, test_set, 2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:24:59.219909Z",
     "iopub.execute_input": "2024-07-09T14:24:59.220316Z",
     "iopub.status.idle": "2024-07-09T14:25:23.437059Z",
     "shell.execute_reply.started": "2024-07-09T14:24:59.220285Z",
     "shell.execute_reply": "2024-07-09T14:25:23.435648Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\nmodel = train_model(model, train_set, test_set, 50)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:30:05.703675Z",
     "iopub.execute_input": "2024-07-09T14:30:05.704072Z",
     "iopub.status.idle": "2024-07-09T14:39:37.106389Z",
     "shell.execute_reply.started": "2024-07-09T14:30:05.704027Z",
     "shell.execute_reply": "2024-07-09T14:39:37.105259Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load('/kaggle/input/modelsecond/best_model_3.pth'))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T15:19:05.103716Z",
     "iopub.execute_input": "2024-07-09T15:19:05.104592Z",
     "iopub.status.idle": "2024-07-09T15:19:05.519312Z",
     "shell.execute_reply.started": "2024-07-09T15:19:05.104560Z",
     "shell.execute_reply": "2024-07-09T15:19:05.518278Z"
    },
    "trusted": true
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Filter implement",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import cv2 as cv\nimport torch\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport dlib\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\ntransform_pred = A.Compose([\n    A.Resize(height=224, width=224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\ndef extract_index_nparray(nparray):\n    index = None\n    for num in nparray[0]:\n        index = num\n        break\n    return index\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass Filter():\n    def __init__(self, model):\n        self.model = model.to(device)\n        self.face_detect = dlib.get_frontal_face_detector()\n        self.filter_landmarks = None\n        self.triangle_list = None\n\n    def detect_landmark(self, image):\n        self.model.eval()\n\n        # Covert BGR to RGB\n        try:\n            image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n        except:\n            pass\n        h, w, c = image.shape\n        # Process image to suitable for model\n        process_image = transform_pred(image=image)\n        # plt.imshow(process_image['image'].cpu().clone().permute(1, 2, 0))\n        process_image = process_image['image'].unsqueeze(0)\n        process_image = process_image.to(device)\n\n        # Get output from model\n        output = self.model(process_image)\n        output = output.view(68, 2)\n\n        landmarks = (output + 0.5)\n        landmarks = landmarks.detach().cpu().numpy()\n        landmarks[:, 0] = landmarks[:, 0] * w\n        landmarks[:, 1] = landmarks[:, 1] * h\n\n\n        return landmarks\n    def get_filter_landmarks_and_delaunay_triangle(self, filter):\n        landmarks = []\n        indexes_triangles = []\n        img_gray = cv.cvtColor(filter, cv.COLOR_BGR2GRAY)\n        faces = self.face_detect(img_gray)\n        for face in faces:\n            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n            landmarks = self.detect_landmark(filter[y: y + h, x : x + w])\n            landmarks += np.array([x, y])\n            landmarks = np.append(landmarks, [[x + 15, y - 10]], axis=0)\n            landmarks = np.append(landmarks, [[x + w - 15, y - 10]], axis=0)\n            points = np.array(landmarks, dtype=np.int32)\n\n            # Delaunay Area\n            convexhull = cv.convexHull(points)\n            bounding_box = cv.boundingRect(convexhull)\n            subdiv2 = cv.Subdiv2D(bounding_box)\n            subdiv2.insert(landmarks)\n            triangles = subdiv2.getTriangleList()\n            triangles = np.array(triangles, dtype=np.int32)\n\n            for t in triangles:\n                pt1 = (t[0], t[1])\n                pt2 = (t[2], t[3])\n                pt3 = (t[4], t[5])\n\n                id_pt1 = np.where((points == pt1).all(axis=1))\n                id_pt1 = extract_index_nparray(id_pt1)\n\n                id_pt2 = np.where((points == pt2).all(axis=1))\n                id_pt2 = extract_index_nparray(id_pt2)\n\n                id_pt3 = np.where((points == pt3).all(axis=1))\n                id_pt3 = extract_index_nparray(id_pt3)\n\n                if id_pt1 is not None and id_pt2 is not None and id_pt3 is not None:\n                    triangle = [id_pt1, id_pt2, id_pt3]\n                    indexes_triangles.append(triangle)\n        return landmarks, indexes_triangles\n\n    def get_filter_landmarks_from_csv(self, filter_cdv):\n        dt = pd.read_csv(\"../data/filter/csv/squid.csv\", header=None)\n        x = dt[1].values\n        y = dt[2].values\n        return np.array([(x1, y1) for x1, y1 in zip(x, y)])\n\n    def get_filter_landmarks_and_delaunay_triangle_csv(self, filter, filter_csv):\n        landmarks = self.get_filter_landmarks_from_csv(filter_csv)\n        landmarks = np.array(landmarks, dtype=np.float32)\n        convexhull = cv.convexHull(landmarks)\n        bounding_box = cv.boundingRect(convexhull)\n        points = np.array(landmarks, dtype=np.int32)\n        subdiv = cv.Subdiv2D(bounding_box)\n        subdiv.insert(landmarks)\n\n        triangles = subdiv.getTriangleList()\n        triangles = np.array(triangles, dtype=np.int32)\n\n        indexes_triangles = []\n\n        for t in triangles:\n            pt1 = (t[0], t[1])\n            pt2 = (t[2], t[3])\n            pt3 = (t[4], t[5])\n\n            id_pt1 = np.where((points == pt1).all(axis=1))\n            id_pt1 = extract_index_nparray(id_pt1)\n\n            id_pt2 = np.where((points == pt2).all(axis=1))\n            id_pt2 = extract_index_nparray(id_pt2)\n\n            id_pt3 = np.where((points == pt3).all(axis=1))\n            id_pt3 = extract_index_nparray(id_pt3)\n\n            if id_pt1 is not None and id_pt2 is not None and id_pt3 is not None:\n                triangle = [id_pt1, id_pt2, id_pt3]\n                indexes_triangles.append(triangle)\n\n        return landmarks, indexes_triangles\n\n    def apply_filter(self, image, filter, image_landmarks, filter_landmarks, triangle_list):\n        try:\n            img2 = image\n            img = filter\n            img2_new_face = np.zeros_like(img2)\n            img2_gray = cv.cvtColor(img2, cv.COLOR_BGR2GRAY)\n            landmarks_points = np.array(filter_landmarks, dtype=np.int32)\n            landmarks_points2 = np.array(image_landmarks, dtype=np.int32)\n            convex_hull = cv.convexHull(landmarks_points)\n            convex_hull2 = cv.convexHull(landmarks_points2)\n            for triangle_index in triangle_list:\n                # Face 1\n                # Buoc 1: Xac dinh tung tam giac\n                tri_pt1 = landmarks_points[triangle_index[0]]\n                tri_pt2 = landmarks_points[triangle_index[1]]\n                tri_pt3 = landmarks_points[triangle_index[2]]\n                triangle = np.array([tri_pt1, tri_pt2, tri_pt3], dtype=np.int32)\n\n                # Buoc 2: Xac dinh vi tri va phan bounding cua moi tam giac\n                bounding_rect1 = cv.boundingRect(triangle)\n                (x, y, w, h) = bounding_rect1\n                # cv.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 1)\n                crop_image1 = img[y:y + h, x:x + w]\n                crop_image1_mask = np.zeros((h, w), dtype=np.uint8)\n                points = np.array([[tri_pt1[0] - x, tri_pt1[1] - y],\n                                   [tri_pt2[0] - x, tri_pt2[1] - y],\n                                   [tri_pt3[0] - x, tri_pt3[1] - y]], np.int32)\n\n                # Buoc 3: Tao ra 1 mask giup xac dinh chinh xac vi tri tam giac duoc chuyen doi de khong anh huong den cac pixel khac\n                cv.fillConvexPoly(crop_image1_mask, points, 255)\n                crop_image1 = cv.bitwise_and(crop_image1, crop_image1, mask=crop_image1_mask)\n                # cv.line(img, tri_pt1, tri_pt2, (0, 0, 255), 2)\n                # cv.line(img, tri_pt3, tri_pt2, (0, 0, 255), 2)\n                # cv.line(img, tri_pt1, tri_pt3, (0, 0, 255), 2)\n\n                # Face 2\n                tri2_pt1 = landmarks_points2[triangle_index[0]]\n                tri2_pt2 = landmarks_points2[triangle_index[1]]\n                tri2_pt3 = landmarks_points2[triangle_index[2]]\n\n                triangle2 = np.array([tri2_pt1, tri2_pt2, tri2_pt3], dtype=np.int32)\n\n                bounding_rect2 = cv.boundingRect(triangle2)\n                (x, y, w, h) = bounding_rect2\n                # cv.rectangle(img2, (x, y), (x + w, y + h), (0, 255, 0), 1)\n                crop_image2 = img2[y:y + h, x:x + w]\n                crop_image2_mask = np.zeros((h, w), dtype=np.uint8)\n                points2 = np.array([[tri2_pt1[0] - x, tri2_pt1[1] - y],\n                                    [tri2_pt2[0] - x, tri2_pt2[1] - y],\n                                    [tri2_pt3[0] - x, tri2_pt3[1] - y]], np.int32)\n                cv.fillConvexPoly(crop_image2_mask, points2, 255)\n                crop_image2 = cv.bitwise_and(crop_image2, crop_image2, mask=crop_image2_mask)\n\n                # cv.line(img2, tri2_pt1, tri2_pt2, (0, 0, 255), 2)\n                # cv.line(img2, tri2_pt3, tri2_pt2, (0, 0, 255), 2)\n                # cv.line(img2, tri2_pt1, tri2_pt3, (0, 0, 255), 2)\n\n                cropped_tr2_mask = np.zeros((h, w), np.uint8)\n                cv.fillConvexPoly(cropped_tr2_mask, points2, 255)\n\n                points = np.float32(points)\n                points2 = np.float32(points2)\n\n                # Buoc 4: Thuc hien chuyen doi giua 2 tam giac\n                # getAffine nhan vao chinh xac 3 diem de tao ma tran chuyen doi\n\n                M = cv.getAffineTransform(points, points2)\n                crop_trans = cv.warpAffine(crop_image1, M, (w, h), flags=cv.INTER_NEAREST)\n                crop_trans = cv.bitwise_and(crop_trans, crop_trans, mask=cropped_tr2_mask)\n\n                img2_new_face_area = img2_new_face[y: y + h, x: x + w]\n                img2_new_face_area_gray = cv.cvtColor(img2_new_face_area, cv.COLOR_BGR2GRAY)\n                _, mask_designed = cv.threshold(img2_new_face_area_gray, 1, 255, cv.THRESH_BINARY_INV)\n                crop_trans = cv.bitwise_and(crop_trans, crop_trans, mask=mask_designed)\n                img2_new_face_area = cv.add(img2_new_face_area, crop_trans)\n                img2_new_face[y: y + h, x: x + w] = img2_new_face_area\n\n            img2_face_mask = np.zeros_like(img2_gray)\n            img2_head_mask = cv.fillConvexPoly(img2_face_mask, convex_hull2, 255)\n            img2_face_mask = cv.bitwise_not(img2_head_mask)\n\n            img2_noface = cv.bitwise_and(img2, img2, mask=img2_face_mask)\n            result = cv.add(img2_noface, img2_new_face)\n\n            # Adjust color\n            (x, y, w, h) = cv.boundingRect(convex_hull2)\n            center_face2 = (int((x + x + w) / 2), int((y + y + h) / 2))\n            seamlessclone = cv.seamlessClone(result, img2, img2_head_mask, center_face2, cv.NORMAL_CLONE)\n            return seamlessclone\n            # seamlessclone = cv.resize(seamlessclone, (500, 500), interpolation=cv.INTER_AREA)\n            # cv.imshow(\"Image_Face 1\", face_image_1)\n            # cv.imshow(\"Image2\", img2)\n            # cv.imshow(\"Image_Face 2\", face_image_2)\n            # cv.imshow(\"Crop Image 1\", crop_image1)\n            # cv.imshow(\"Crop Image 2\", crop_image2)\n            # cv.imshow(\"Crop Transform\", crop_trans)\n            # cv.imshow(\"New face\", img2_new_face)\n            # cv.imshow(\"Result\", seamlessclone)\n            # cv.waitKey(0)\n        except:\n            return image\n\n    def filter_camera(self, filter, filter_csv=None):\n        cap = cv.VideoCapture(0)\n        while True:\n            _, frame = cap.read()\n            frame = self.filter_image(img=frame, filter=filter, filter_csv=filter_csv)\n\n            cv.imshow(\"Camera Filter\", frame)\n\n            if cv.waitKey(1) & 0xFF == ord('d'):\n                break\n\n    def filter_image(self, img, filter, filter_csv=None):\n        faces = self.face_detect(img)\n        if self.filter_landmarks is None and self.triangle_list is None:\n            if filter_csv is None:\n                self.filter_landmarks, self.triangle_list = self.get_filter_landmarks_and_delaunay_triangle(filter)\n            else:\n                self.filter_landmarks, self.triangle_list = self.get_filter_landmarks_and_delaunay_triangle_csv(filter, filter_csv)\n        for face in faces:\n            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n            image_landmarks = self.detect_landmark(img[y: y + h, x: x + w])\n            image_landmarks += np.array([x, y])\n            image_landmarks = np.append(image_landmarks, [[x + 15, y - 10]], axis=0)\n            image_landmarks = np.append(image_landmarks, [[x + w - 15, y - 10]], axis=0)\n\n            img = self.apply_filter(img, filter, image_landmarks, self.filter_landmarks, self.triangle_list)\n\n        return img",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:55:30.464176Z",
     "iopub.execute_input": "2024-07-09T14:55:30.464621Z",
     "iopub.status.idle": "2024-07-09T14:55:30.517577Z",
     "shell.execute_reply.started": "2024-07-09T14:55:30.464591Z",
     "shell.execute_reply": "2024-07-09T14:55:30.516564Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "filterApp = Filter(model=model)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T15:19:12.738969Z",
     "iopub.execute_input": "2024-07-09T15:19:12.739638Z",
     "iopub.status.idle": "2024-07-09T15:19:13.069694Z",
     "shell.execute_reply.started": "2024-07-09T15:19:12.739599Z",
     "shell.execute_reply": "2024-07-09T15:19:13.068903Z"
    },
    "trusted": true
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "image = cv.imread(\"/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset/helen/trainset/100040721_2.jpg\")\nfilter_image = cv.imread(\"/kaggle/input/mask-joker/joker.jfif\")\n\nimage_filter = filterApp.filter_image(img=image, filter=filter_image)\nplt.imshow(image_filter)\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T15:19:15.322938Z",
     "iopub.execute_input": "2024-07-09T15:19:15.323823Z",
     "iopub.status.idle": "2024-07-09T15:19:15.944591Z",
     "shell.execute_reply.started": "2024-07-09T15:19:15.323784Z",
     "shell.execute_reply": "2024-07-09T15:19:15.943674Z"
    },
    "trusted": true
   },
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pip install opencv-python",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T14:59:14.134487Z",
     "iopub.execute_input": "2024-07-09T14:59:14.135306Z",
     "iopub.status.idle": "2024-07-09T14:59:26.670803Z",
     "shell.execute_reply.started": "2024-07-09T14:59:14.135264Z",
     "shell.execute_reply": "2024-07-09T14:59:26.669490Z"
    },
    "trusted": true
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
